{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189e22a5",
   "metadata": {},
   "source": [
    "# Datalaoder-Based Relation Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b365aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d918d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "environ['DATA_DIR_VG_RCNN'] = '/home/zhanwen/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f3f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.modeling.detector import build_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497d7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import manual_seed as torch_manual_seed\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.cuda import max_memory_allocated, set_device, manual_seed_all\n",
    "from torch.backends import cudnn\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch_manual_seed(seed)\n",
    "    manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.deterministic = True\n",
    "    \n",
    "setup_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7ee7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set args\n",
    "# PRETRAINED\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "\n",
    "MODEL_NAME = '44663493_vctree_baseline_predcls_4GPU_riv_1_copied'\n",
    "CONFIG_FILE = '/home/zhanwen/gsc/checkpoints/44663493_vctree_baseline_predcls_4GPU_riv_1_copied/config.yml'\n",
    "PROJECT_DIR = '/home/zhanwen/gsc'\n",
    "SEED=1234\n",
    "BATCH_SIZE=1\n",
    "\n",
    "cfg.merge_from_file(CONFIG_FILE)\n",
    "cfg.SOLVER.IMS_PER_BATCH = BATCH_SIZE\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "cfg.GLOVE_DIR = f'{PROJECT_DIR}/datasets/vg/'\n",
    "cfg.MODEL.PRETRAINED_DETECTOR_CKPT = f'{PROJECT_DIR}/checkpoints/pretrained_faster_rcnn/model_final.pth'\n",
    "cfg.OUTPUT_DIR = f'{PROJECT_DIR}/checkpoints/{MODEL_NAME}'\n",
    "cfg.PATHS_DATA = f'{PROJECT_DIR}/maskrcnn_benchmark/data/datasets'\n",
    "cfg.OUTPUT_DIR = '/home/zhanwen/gsc/checkpoints/44663493_vctree_baseline_predcls_4GPU_riv_1_copied'\n",
    "cfg.MODEL.WEIGHT = f'{PROJECT_DIR}/checkpoints/{MODEL_NAME}/model_0014000.pth'\n",
    "cfg.PATHS_CATALOG = '/home/zhanwen/gsc/maskrcnn_benchmark/config/paths_catalog.py'\n",
    "\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327af72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VCTreePredictor'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_RELATION_HEAD.PREDICTOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627ff4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename as os_path_basename\n",
    "from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\n",
    "\n",
    "from os.path import exists as os_path_exists\n",
    "from torch import save as torch_save, load as torch_load\n",
    "\n",
    "# MODEL_NAME = 'model.pt'\n",
    "# if os_path_exists(MODEL_NAME):\n",
    "#     model = torch_load(MODEL_NAME, map_location='cpu')\n",
    "# else:\n",
    "#     model = build_detection_model(cfg)\n",
    "#     model_checkpoint_name = os_path_basename(cfg.MODEL.WEIGHT)\n",
    "#     checkpointer = DetectronCheckpointer(cfg, model, save_dir=cfg.OUTPUT_DIR)\n",
    "#     _ = checkpointer.load(cfg.MODEL.WEIGHT)\n",
    "#     torch_save(model, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f9dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists as os_path_exists\n",
    "from torch import save as torch_save, load as torch_load\n",
    "\n",
    "# MODEL_NAME = 'model.pt'\n",
    "# if os_path_exists(MODEL_NAME):\n",
    "#     model = torch_load(MODEL_NAME, map_location='cpu')\n",
    "# else:\n",
    "#     model = build_detection_model(cfg)\n",
    "#     torch_save(model, MODEL_NAME)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5fed7a",
   "metadata": {},
   "source": [
    "# Deliverable: Walk through it once and compute the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b7da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the dataset statistics computed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be549869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images Per GPU: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  train\n",
      "root_classes_count:  {}\n",
      "mean root class number:  0.0\n",
      "sum root class number:  0\n",
      "leaf_classes_count:  {}\n",
      "mean leaf class number:  0.0\n",
      "sum leaf class number:  0\n",
      "all_classes_count:  {}\n",
      "mean all class number:  0.0\n",
      "sum all class number:  0\n",
      "number images:  57723\n"
     ]
    }
   ],
   "source": [
    "from maskrcnn_benchmark.data import make_data_loader\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "        cfg,\n",
    "        mode='train',\n",
    "        is_distributed=False,\n",
    "        start_iter=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02437f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log as np_log\n",
    "\n",
    "from maskrcnn_benchmark.data import get_dataset_statistics # calls VisualGenome.get_statistics which calls get_VG_statistics\n",
    "\n",
    "statistics = get_dataset_statistics(cfg)\n",
    "\n",
    "fg_matrix = statistics['fg_matrix']\n",
    "eps = 1e-3\n",
    "\n",
    "pred_dist = np_log(fg_matrix / fg_matrix.sum(2)[:, :, None] + eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a3814",
   "metadata": {},
   "source": [
    "# Let's Manually Calculate the correlations from the foreground matrix which is \n",
    "\n",
    "```python\n",
    "fg_matrix = np_zeros((num_obj_classes, num_obj_classes, num_rel_classes), dtype=np_int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018b72fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151, 151, 51])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_matrix.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92b8e0",
   "metadata": {},
   "source": [
    "# Let's define relation-relation correlation\n",
    "\n",
    "1. Relation Word Embedding Similarity\n",
    "2. Relation Co-occurrence\n",
    "3. Relation covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57474e",
   "metadata": {},
   "source": [
    "## 1. Relation Word Embedding Similarity\n",
    "\n",
    "```python\n",
    "for i in idx2label_177:\n",
    "    for j in idx2label_177:\n",
    "        if i == j:\n",
    "            continue\n",
    "        a = wn.synset(label2synset_combined[idx2label_177[i]])\n",
    "        b = wn.synset(label2synset_combined[idx2label_177[j]])\n",
    "        s1 = a.path_similarity(b)\n",
    "        s2 = a.lch_similarity(b) / 3.6375861597263857\n",
    "        s3 = a.wup_similarity(b)\n",
    "        word_sim[:, int(i), int(j)] = [s1, s2, s3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24093742",
   "metadata": {},
   "source": [
    "# 2. Relation Co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e4ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = fg_matrix.sum((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d43b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol2 = lol[:, None] / lol[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "064ea905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 51])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol2.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6d4c7",
   "metadata": {},
   "source": [
    "# Naive Relation Augmentation Strategies\n",
    "\n",
    "1. Relation augmentation given pred (like the bar plots)\n",
    "2. Relation augmentation given subj, pred\n",
    "3. Relation augmentation given obj, pred\n",
    "4. Relation augmentation given subj, obj, pred\n",
    "5. Relation augmentation given pred2pred knowledge (word embedding) with an alpha control\n",
    "6. Relation augmentation given pred2pred knowledge (commonsense knowledge - just ConceptNet) with a beta control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422f102",
   "metadata": {},
   "source": [
    "# 1. Relation augmentation given pred (like the bar plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a510e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_counts = fg_matrix.sum((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fde13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_freq_pairwise = pred_counts[:, None] / pred_counts[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e16974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import empty_like as torch_empty_like\n",
    "# pred2pred = torch_zeros(51, 51)\n",
    "\n",
    "# for i in range(51):\n",
    "#     for j in range(51):\n",
    "#         preds_ij = fg_matrix[i, j].float()\n",
    "        \n",
    "#         print('before, preds_ij =', preds_ij)\n",
    "# #         print(preds_ij)\n",
    "        \n",
    "#         preds_ij = cov[i, j, :] = preds_ij/preds_ij.sum()\n",
    "#         # PyTorch impl\n",
    "# #         m = preds_ij.mean()\n",
    "# #         s = preds_ij.std(un|biased=False)\n",
    "# #         preds_ij -= m\n",
    "# #         preds_ij /= s\n",
    "#         print('after, preds_ij =', preds_ij)\n",
    "#         print('after, preds_ij.sum() =', preds_ij.sum())\n",
    "#         pred2pred[]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f11cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fg_matrix.float().cov((0,1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "024fb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_cov(fg_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5aa5d",
   "metadata": {},
   "source": [
    "# 3. Relation Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ce019",
   "metadata": {},
   "source": [
    "# 4. Knowledge-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd64c1f",
   "metadata": {},
   "source": [
    "# Question: How did others (like KERN and GB-Net) get their correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f58ea00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2bbd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_data_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25de79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target, index = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5da3df83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3,  20,  49,  49,  57,  58,  59,  97,  99, 105, 111, 115,  77,  78])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = target.extra_fields['labels']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d1088c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, 50,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 31,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 20,  0, 21,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = target.extra_fields['relation']\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a703bf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 50, 31, 20, 21])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[relations.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4fc80e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 11, 12, 13, 13])\n",
      "tensor([ 0,  8, 10,  5,  7])\n",
      "(tensor([ 1, 11, 12, 13, 13]), tensor([ 0,  8, 10,  5,  7]))\n"
     ]
    }
   ],
   "source": [
    "subj_id, obj_id = relations_idx = relations.nonzero(as_tuple=True)\n",
    "print(subj_id)\n",
    "print(obj_id)\n",
    "print(relations_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb5aad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as os_path_join\n",
    "from os import environ as os_environ\n",
    "\n",
    "DATA_DIR = os_environ['DATA_DIR_VG_RCNN']\n",
    "\n",
    "with open(os_path_join(DATA_DIR, 'visual_genome', 'VG-SGG-dicts-with-attri.json'), 'r') as fin:\n",
    "    scene_graph_meta = json.load(fin)\n",
    "labels = list(scene_graph_meta['label_to_idx'].keys())\n",
    "preds = list(scene_graph_meta['predicate_to_idx'].keys())\n",
    "\n",
    "# TODO: limit to tail classes for freq. Because if I just do a freq based, I'm gonna \n",
    "# make it worse because we are gonna get the higher freq ones for aug triplets.\n",
    "# or just invert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e47114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2preds = ['_'] + list(scene_graph_meta['idx_to_predicate'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5275528",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2preds_np = np.array(idx2preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11ec344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2preds_np[triplets_new[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0575e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from maskrcnn_benchmark.structures.image_list import ImageList\n",
    "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
    "\n",
    "\n",
    "\n",
    "def rel_aug(images: ImageList, targets: Tuple[BoxList], num_to_aug: int, strategy: str) -> Tuple[ImageList, Tuple[BoxList]]:\n",
    "    p_rel_all = pred_counts/pred_counts.sum()\n",
    "    if strategy == 'all':\n",
    "        return rel_aug_all(images, targets, p_rel_all, num_to_aug)\n",
    "    else:\n",
    "        raise NotImplementedError(f'rel_aug: strategy={strategy} is not implemented yet')\n",
    "\n",
    "\n",
    "from torch import as_tensor as torch_as_tensor\n",
    "# def rel_aug_all_triplets(triplet: Tuple[Tensor, Tensor, Tensor], num2aug: int, replace: bool) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "def rel_aug_all_triplets(idx_rel: int, num2aug: int, replace: bool) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    '''\n",
    "    Given a single triplet in the form of (subj, rel, obj) and outputs a list of triplets with\n",
    "    the , not including the original. You should append the original.\n",
    "    \n",
    "    Note that the input indices much be singular (one element) but the outputs will have multiple\n",
    "    (specificall num2aug elements).\n",
    "    '''\n",
    "    \n",
    "    # Construct the inverse relation frequency distribution\n",
    "    \n",
    "    n=len(pred_counts)\n",
    "    P_REL_ALL = 1- (pred_counts/pred_counts.sum()).repeat(n, 1)\n",
    "    DIST_RELS_ALL_EXCLUDED_BY = P_REL_ALL.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1) # remove diagonal values\n",
    "\n",
    "#     idxs_subj = []\n",
    "#     idxs_obj = []\n",
    "#     idxs_rel = []\n",
    "    \n",
    "#     idx_subj, idx_rel, idx_obj = triplet\n",
    "    \n",
    "    idx_chosen = DIST_RELS_ALL_EXCLUDED_BY[idx_rel].multinomial(num2aug, replacement=replace)\n",
    "    return idx_chosen\n",
    "#     idxs_subj.append([idx_subj for _ in range(num2aug)])\n",
    "#     idxs_obj.append([idx_obj for _ in range(num2aug)])\n",
    "#     idxs_rel.extend(idx_chosen)\n",
    "#     return torch_as_tensor(idxs_subj), torch_as_tensor(idxs_rel), torch_as_tensor(idxs_obj)\n",
    "\n",
    "def rel_aug_each_instance(triplet: Tuple[Tensor, Tensor, Tensor], num2aug: int, replace: bool) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    '''\n",
    "    Given a single triplet in the form of (subj, rel, obj) and outputs a list of triplets with\n",
    "    the , not including the original. You should append the original.\n",
    "    \n",
    "    Note that the input indices much be singular (one element) but the outputs will have multiple\n",
    "    (specificall num2aug elements).\n",
    "    '''\n",
    "    \n",
    "    # Construct the inverse relation frequency distribution\n",
    "    \n",
    "    n=len(pred_counts)\n",
    "    P_REL_ALL = 1- (pred_counts/pred_counts.sum()).repeat(n, 1)\n",
    "    DIST_RELS_ALL_EXCLUDED_BY = P_REL_ALL.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1) # remove diagonal values\n",
    "\n",
    "    idxs_subj = []\n",
    "    idxs_obj = []\n",
    "    idxs_rel = []\n",
    "    \n",
    "    idx_subj, idx_rel, idx_obj = triplet\n",
    "    \n",
    "    idx_chosen = DIST_RELS_ALL_EXCLUDED_BY[idx_rel].multinomial(num2aug, replacement=replace)\n",
    "    return idx_chosen\n",
    "#     idxs_subj.append([idx_subj for _ in range(num2aug)])\n",
    "#     idxs_obj.append([idx_obj for _ in range(num2aug)])\n",
    "#     idxs_rel.extend(idx_chosen)\n",
    "#     return torch_as_tensor(idxs_subj), torch_as_tensor(idxs_rel), torch_as_tensor(idxs_obj)\n",
    "\n",
    "def rel_aug_all(images: ImageList, targets: Tuple[BoxList], dist_rels_all:Tensor, num2aug: int, replace: bool) -> Tuple[ImageList, Tuple[BoxList]]:\n",
    "    '''\n",
    "    dist_preds_all:Tensor: the frequency distribution of all relations\n",
    "    num2aug:int: \n",
    "    \n",
    "    '''\n",
    "    batch_size = image.size()[0]\n",
    "    # pick up the num_examples draw given distribution of similar \n",
    "    \n",
    "    # 1. Exclude iteratively or better, build cached matrix of excluded distributions by index.\n",
    "    \n",
    "#     for \n",
    "    n = preds_count.size(0)\n",
    "    dist_rels_all_excluded_by = dist_rels_all.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1) # remove diagonal values\n",
    "\n",
    "    # Exclude head relation classes or include tail relation classes.\n",
    "    triplets_augmented = []\n",
    "    for t in triplets:\n",
    "        idx_subj, idx_obj, idx_rel = t\n",
    "        idx_chosen = dist_rels_all_excluded_by[idx_rel].multinomial(num2aug, replacement=replace)\n",
    "        # Actually idx_chosen is the new array of augmentation triplets\n",
    "        # Augment here\n",
    "#         triplets_chosen = \n",
    "        # same objects.\n",
    "        triplets_augmented.extend(triplets_chosen)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "\n",
    "\n",
    "# def mixgen_batch(image, text, num, lam=0.5):\n",
    "#     batch_size = image.size()[0]\n",
    "#     index = np.random.permutation(batch_size)\n",
    "#     for i in range(batch_size):\n",
    "#         # image mixup\n",
    "#         image[i,:] = lam * image[i,:] + (1 - lam) * image[index[i],:]\n",
    "#         # text concat\n",
    "#         text[i] = text[i] + \" \" + text[index[i]]\n",
    "#     return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d51ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch import no_grad as torch_no_grad, cat as torch_cat, as_tensor as torch_as_tensor\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "\n",
    "\n",
    "# for iteration, (images, targets, _) in tqdm(enumerate(train_data_loader)):\n",
    "#     if iteration > 100: break\n",
    "\n",
    "\n",
    "def augment_targets(images, targets):\n",
    "    # TODO: vectorize\n",
    "#     relations = [target.extra_fields['relation'] for target in targets]\n",
    "#     labels = [target.extra_fields['labels'] for target in targets]\n",
    "    images_augmented = []\n",
    "    targets_augmented = []\n",
    "    for image, target in zip(images.tensors, targets):\n",
    "        relation_old = target.extra_fields['relation']\n",
    "    \n",
    "        idx_subj, idx_obj = idx_rel = relation_old.nonzero(as_tuple=True) # tuple\n",
    "        rels = relation_old[idx_rel]\n",
    "        # Future idea: relation localization. Why are objects locally & globally indexed \n",
    "        # but relations only globally indexed? Because of bbox?\n",
    "\n",
    "        # triplets are represented as the relation map.\n",
    "\n",
    "        # rels is essentially triplets\n",
    "\n",
    "#         idxs_subj = []\n",
    "#         idxs_rel = []\n",
    "#         idxs_obj = []\n",
    "        \n",
    "#         images_from_image = []\n",
    "#         targets_from_target = []\n",
    "        images_augmented.append(image)\n",
    "        targets_augmented.append(target)\n",
    "\n",
    "\n",
    "        for idx_subj_og, rel_og, idx_obj_og in zip(idx_subj, rels, idx_obj):\n",
    "#             idx_subj, idx_rel, idx_obj = rel_aug_all_triplets(triplets_old, 10, True)\n",
    "\n",
    "            # First add old\n",
    "#             images_from_image.append(image)\n",
    "            # TODO: how to append to an ImageList? Use to_image_list to convert torch tensors\n",
    "#             targets_from_target.append(target)\n",
    "            # TODO: how to append to targets? It's a list of boxlists. So might be easier.\n",
    "\n",
    "#             triplets_new = rel_aug_all_triplets(triplet_old, 10, True)\n",
    "#             triplets_new = rel_aug_all_triplets(rel_og, 10, True)\n",
    "            rels_new = rel_aug_all_triplets(rel_og, 10, True)\n",
    "            print(len(rels_new))\n",
    "            for rel_new in rels_new:\n",
    "#                 images_from_image.append(image)\n",
    "                images_augmented.append(image)\n",
    "                \n",
    "                # NOTE on meditation and working and reducing anxiety and zen: remember\n",
    "                # doing math proofs in the downstairs apartment? How calm and focused I was?\n",
    "                # like Xin Ru Zhi Shui.\n",
    "                \n",
    "                # Only idx_rel is new here.\n",
    "#                 idx_obj, idx_rel, idx_subj = triplet_new\n",
    "                \n",
    "                # Triplet to Map\n",
    "                relation_new = relation_old.detach().clone()\n",
    "#                 target_new = target.detach().clone()\n",
    "                target_new = deepcopy(target)\n",
    "                relation_new[idx_subj_og, idx_subj_og] = rel_new\n",
    "                target_new.extra_fields['relation'] = relation_new\n",
    "#                 targets_from_target.append(target_new)\n",
    "#                 targets_from_target.append(target_new)\n",
    "                targets_augmented.append(target_new)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return to_image_list(images_augmented), targets_augmented\n",
    "                \n",
    "                # Do we really need to recreate relation_old from zeros? I don't think so. \n",
    "                \n",
    "#                 (x == 0).nonzero()\n",
    "#                 x = torch.empty(5, 5).random_(3)\n",
    "#                 idx = x.nonzero()\n",
    "#                 y = torch.zeros(5, 5)\n",
    "#                 y[idx[:,0], idx[:,1]] = x[idx[:, 0], idx[:, 1]]\n",
    "#                 print((x==y).all())\n",
    "                \n",
    "#             idxs_subj.append(idx_subj)\n",
    "#             idxs_rel.append(idx_rel)\n",
    "#             idxs_obj.append(idx_obj)\n",
    "            \n",
    "            # For single-label augmentation, each iteration in this loop is an \"extra\" image and target\n",
    "            \n",
    "\n",
    "#         idxs_subj = torch_cat(idxs_subj)\n",
    "#         idxs_rel = torch_cat(idxs_rel)\n",
    "#         idxs_obj = torch_cat(idxs_obj)\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "        # TODO: convert matrices back into map like relation_old [num_obj, num_obj]\n",
    "        # currently, these maps are singular in terms of relations. We cannot have multiple.\n",
    "        # perhaps we need multilabel anyway? One way to do this is to borrow from the MLSGG codebase.\n",
    "        # Alternatively, output multiple maps with the same images and targets but with different gt map.\n",
    "        \n",
    "        # But doesn't single-label run into problem of bad gt answers? Can both be correct? I'm not so sure.\n",
    "        \n",
    "        \n",
    "#         target.extra_fields['relation'] = \n",
    "\n",
    "#     return targets\n",
    "\n",
    "    # where is the ground truth relation labels? There's definitely some in get_dataset_statistics\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a42cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "> \u001b[0;32m/tmp/ipykernel_37895/1539722718.py\u001b[0m(4)\u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      2 \u001b[0;31m\u001b[0;31m# to_image_list = ImageList.to_image_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 4 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m\u001b[0;31m#     import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m    \u001b[0mimages_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(targets_new)\n",
      "41\n",
      "ipdb> q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:09, ?it/s]\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from maskrcnn_benchmark.structures.image_list import ImageList\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# to_image_list = ImageList.to_image_list\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration, (images, targets, _) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_data_loader)):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     images_new, targets_new \u001b[38;5;241m=\u001b[39m augment_targets(images, targets)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     images_new = targets.to_image_list(images_new)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from maskrcnn_benchmark.structures.image_list import ImageList\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# to_image_list = ImageList.to_image_list\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration, (images, targets, _) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_data_loader)):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     images_new, targets_new \u001b[38;5;241m=\u001b[39m augment_targets(images, targets)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     images_new = targets.to_image_list(images_new)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sgb/lib/python3.8/bdb.py:88\u001b[0m, in \u001b[0;36mBdb.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;66;03m# None\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_call(frame, arg)\n",
      "File \u001b[0;32m~/miniconda3/envs/sgb/lib/python3.8/bdb.py:113\u001b[0m, in \u001b[0;36mBdb.dispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_here(frame) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreak_here(frame):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_line(frame)\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquitting: \u001b[38;5;28;01mraise\u001b[39;00m BdbQuit\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from maskrcnn_benchmark.structures.image_list import ImageList\n",
    "# to_image_list = ImageList.to_image_list\n",
    "\n",
    "for iteration, (images, targets, _) in tqdm(enumerate(train_data_loader)):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    images_new, targets_new = augment_targets(images, targets)\n",
    "#     images_new = targets.to_image_list(images_new)\n",
    "#     import pdb; pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddffc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# MixGen: A New Multi-Modal Data Augmentation\n",
    "# https://arxiv.org/abs/2206.08358\n",
    "# Apache-2.0 License, Copyright 2022 Amazon\n",
    "# \"\"\"\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def mixgen(image, text, num, lam=0.5):\n",
    "#     # default MixGen\n",
    "#     for i in range(num):\n",
    "#         # image mixup\n",
    "#         image[i,:] = lam * image[i,:] + (1 - lam) * image[i+num,:]\n",
    "#         # text concat\n",
    "#         text[i] = text[i] + \" \" + text[i+num]\n",
    "#     return image, text\n",
    "\n",
    "# def mixgen(image, text, num, lam=0.5):\n",
    "#     # default MixGen\n",
    "#     for i in range(num):\n",
    "#         # image mixup\n",
    "#         image[i,:] = lam * image[i,:] + (1 - lam) * image[i+num,:]\n",
    "#         # text concat\n",
    "#         text[i] = text[i] + \" \" + text[i+num]\n",
    "#     return image, text\n",
    "\n",
    "# >>> from random import choices\n",
    "# >>> population = [1, 2, 3, 4, 5, 6]\n",
    "# >>> weights = [0.1, 0.05, 0.05, 0.2, 0.4, 0.2]\n",
    "# >>> choices(population, weights)\n",
    "# [4]\n",
    "\n",
    "\n",
    "\n",
    "# # https://discuss.pytorch.org/t/torch-equivalent-of-numpy-random-choice/16146/14\n",
    "# a = np.array([1, 2, 3, 4])\n",
    "# p = np.array([0.1, 0.1, 0.1, 0.7])\n",
    "# n = 2\n",
    "# replace = True\n",
    "# b = np.random.choice(a, p=p, size=n, replace=replace)\n",
    "\n",
    "# # Equivalent to\n",
    "\n",
    "# a = torch.tensor([1, 2, 3, 4])\n",
    "# p = torch.tensor([0.1, 0.1, 0.1, 0.7])\n",
    "# n = 2\n",
    "# replace = True\n",
    "# idx = p.multinomial(num_samples=n, replacement=replace)\n",
    "# b = a[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c80f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.data import Dataset\n",
    "\n",
    "# class CachedDataset(Dataset):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import Embedding\n",
    "\n",
    "# emb_obj = Embedding(151, 100)\n",
    "# # emb_relation = Embedding(51, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b40fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import cat as torch_cat\n",
    "\n",
    "# subj_label_gt_concat = torch_cat(subj_label_gt, dim=0)\n",
    "# obj_label_gt_concat = torch_cat(obj_label_gt, dim=0)\n",
    "# relation_gt_concat = torch_cat(relation_gt, dim=0)\n",
    "# union_features_all_concat = torch_cat(union_features_all, dim=0)\n",
    "# roi_features_all_concat = torch_cat(roi_features_all, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8a297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subj_label_gt_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_one_hot = emb_obj(subj_label_gt_concat)\n",
    "# obj_one_hot = emb_obj(obj_label_gt_concat)\n",
    "# # relation_one_hot = emb_relation(relation_gt_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896b533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import DataFrame\n",
    "\n",
    "# df = DataFrame(columns=['relation_pred', 'subj_label_gt', 'obj_label_gt', 'relation_gt', 'union_features', 'roi_features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union_features_all_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e67c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_gt_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = torch_cat((subj_one_hot, obj_one_hot, union_features_all_concat, roi_features_all_concat), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_label_gt_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34aef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(subj_label_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef52d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(roi_features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e27c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['obj_label_gt'] = \n",
    "# df['subj_label_gt'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['roi_features'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4c718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d23188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 151\n",
    "# embedding_size = 10\n",
    "\n",
    "# embedding = nn.Embedding(num_classes, embedding_size)\n",
    "\n",
    "# class_vector = torch.tensor([1, 0, 3, 3, 2])\n",
    "\n",
    "# embedded_classes = embedding(class_vector)\n",
    "# embedded_classes.size() # => torch.Size([5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec985318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import cat as torch_cat\n",
    "# from torch.nn import Module, Linear\n",
    "# from torch.optim import Adam\n",
    "\n",
    "\n",
    "# class LogisticRegressionGD(Module):\n",
    "#     def __init__(self, input_dim, output_dim, embedding_size):\n",
    "#         super().__init__()\n",
    "#         self.emb = Embedding(num_classes, embedding_size)\n",
    "#         self.linear = Linear(input_dim, output_dim)\n",
    "        \n",
    "#     def forward(self, subj_label_gt, obj_label_gt, union_features, roi_features):\n",
    "#         subj = self.emb(subj_label_gt)\n",
    "#         obj = self.emb(obj_label_gt)\n",
    "#         x = torch_cat(subj, obj, union_features, roi_features)\n",
    "#         return self.linear(x)\n",
    "    \n",
    "# loss_fn = BCEWithLogitsLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# model_gd = LogisticRegressionGD()\n",
    "# # preds = model()\n",
    "# # loss = loss_fn(input, target)\n",
    "# # loss.backward()\n",
    "\n",
    "# # for epochs\n",
    "# for inputy, target in dataset:\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model_gd(inputy)\n",
    "#     loss = loss_fn(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edebfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegressionGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "\n",
    "# Result = minimize(fun = logLikelihoodLogit, \n",
    "#                                  x0 = np.array([-.1, -.03, -.01, .44, .92, .53,1.8, .71]), \n",
    "#                                  args = (mX, vY),\n",
    "#                                  method = 'TNC',\n",
    "#                                  jac = likelihoodScore);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9867326",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#         for relation, label in zip(relations, labels)\n",
    "#             subj_label, obj_label = label[subj_idx], label[obj_idx]\n",
    "        \n",
    "#         print(f'#relations = {len(relations[0])}')\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "\n",
    "#         subj_idx, obj_idx = relations_idxs = [relation.nonzero(as_tuple=True) for relation in relations]\n",
    "#         relations_idxs = [relation.nonzero(as_tuple=True) for relation in relations]\n",
    "        \n",
    "#         subj_idx = [relations_idx[0] for relations_idx in relations_idxs]\n",
    "#         obj_idx = [relations_idx[1] for relations_idx in relations_idxs]\n",
    "#         result, rel_pair_idxs, roi_features, union_features = model(images, targets)\n",
    "    \n",
    "#         for pair_idx in rel_pair_idxs:\n",
    "\n",
    "# #             import pdb; pdb.set_trace()\n",
    "\n",
    "# #             subj_idx, obj_idx = relations_idx = relation.nonzero(as_tuple=True)\n",
    "\n",
    "# #             print(f'#relations = {len(relation)}')\n",
    "\n",
    "#             #             import pdb; pdb.set_trace()\n",
    "#             # There are more proposal relations than gt relations\n",
    "#             relation_label = relation[relations_idx]\n",
    "#             # look up: if match. Otherwise 0\n",
    "            \n",
    "            \n",
    "#             subj_label, obj_label = label[subj_idx], label[obj_idx]\n",
    "\n",
    "#             subj_label_gt.append(subj_label.cpu())\n",
    "#             obj_label_gt.append(obj_label.cpu())\n",
    "#             relation_gt.append(relation_label.cpu())\n",
    "    \n",
    "\n",
    "#         for relation, label, pair_idx in zip(relations, labels, rel_pair_idxs):\n",
    "            \n",
    "# #             import pdb; pdb.set_trace()\n",
    "            \n",
    "#             subj_idx, obj_idx = relations_idx = relation.nonzero(as_tuple=True)\n",
    "#             relation_idx_ = relation.nonzero()\n",
    "            \n",
    "#             print(f'#relations = {len(relation)}')\n",
    "            \n",
    "#             relation_label = relation[relations_idx]\n",
    "            \n",
    "#             import pdb; pdb.set_trace()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Proposals\n",
    "#             rel_pair_idx[]\n",
    "            \n",
    "#             for propsed_pair_id in rel_pair_idx:\n",
    "#                 if propsed_pair_id\n",
    "                \n",
    "#             label = index_select(, 1, 0)\n",
    "            \n",
    "#             label_total_with_0 = torch_where(rel_idx_proposed=rel_idx_gt,)\n",
    "\n",
    "\n",
    "#             subj_label_gt.append(subj_label.cpu())\n",
    "#             obj_label_gt.append(obj_label.cpu())\n",
    "#             relation_gt.append(relation_label.cpu())\n",
    "\n",
    "#         roi_features_all.append(roi_features.cpu())\n",
    "#         union_features_all.append(union_features.cpu())\n",
    "        \n",
    "#         torch_cat((roi_features, union_features), dim=-1)\n",
    "\n",
    "\n",
    "#         del subj_label, obj_label, relation_label, roi_features, union_features, images, targets\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgb",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
