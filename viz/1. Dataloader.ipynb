{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af712eb9",
   "metadata": {},
   "source": [
    "# Datalaoder-Based Relation Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b087624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323cc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbeb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "environ['DATA_DIR_VG_RCNN'] = '/home/zhanwen/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9ec5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.modeling.detector import build_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434ba2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import manual_seed as torch_manual_seed\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.cuda import max_memory_allocated, set_device, manual_seed_all\n",
    "from torch.backends import cudnn\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch_manual_seed(seed)\n",
    "    manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.deterministic = True\n",
    "    \n",
    "setup_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f3cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set args\n",
    "# PRETRAINED\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "\n",
    "MODEL_NAME = '44663493_vctree_baseline_predcls_4GPU_riv_1_copied'\n",
    "CONFIG_FILE = '/home/zhanwen/gsc/checkpoints/44663493_vctree_baseline_predcls_4GPU_riv_1_copied/config.yml'\n",
    "PROJECT_DIR = '/home/zhanwen/gsc'\n",
    "SEED=1234\n",
    "BATCH_SIZE=1\n",
    "\n",
    "cfg.merge_from_file(CONFIG_FILE)\n",
    "cfg.SOLVER.IMS_PER_BATCH = BATCH_SIZE\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "cfg.GLOVE_DIR = f'{PROJECT_DIR}/datasets/vg/'\n",
    "cfg.MODEL.PRETRAINED_DETECTOR_CKPT = f'{PROJECT_DIR}/checkpoints/pretrained_faster_rcnn/model_final.pth'\n",
    "cfg.OUTPUT_DIR = f'{PROJECT_DIR}/checkpoints/{MODEL_NAME}'\n",
    "cfg.PATHS_DATA = f'{PROJECT_DIR}/maskrcnn_benchmark/data/datasets'\n",
    "cfg.OUTPUT_DIR = '/home/zhanwen/gsc/checkpoints/44663493_vctree_baseline_predcls_4GPU_riv_1_copied'\n",
    "cfg.MODEL.WEIGHT = f'{PROJECT_DIR}/checkpoints/{MODEL_NAME}/model_0014000.pth'\n",
    "cfg.PATHS_CATALOG = '/home/zhanwen/gsc/maskrcnn_benchmark/config/paths_catalog.py'\n",
    "\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea29d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VCTreePredictor'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_RELATION_HEAD.PREDICTOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb72932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename as os_path_basename\n",
    "from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\n",
    "\n",
    "from os.path import exists as os_path_exists\n",
    "from torch import save as torch_save, load as torch_load\n",
    "\n",
    "# MODEL_NAME = 'model.pt'\n",
    "# if os_path_exists(MODEL_NAME):\n",
    "#     model = torch_load(MODEL_NAME, map_location='cpu')\n",
    "# else:\n",
    "#     model = build_detection_model(cfg)\n",
    "#     model_checkpoint_name = os_path_basename(cfg.MODEL.WEIGHT)\n",
    "#     checkpointer = DetectronCheckpointer(cfg, model, save_dir=cfg.OUTPUT_DIR)\n",
    "#     _ = checkpointer.load(cfg.MODEL.WEIGHT)\n",
    "#     torch_save(model, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c226b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists as os_path_exists\n",
    "from torch import save as torch_save, load as torch_load\n",
    "\n",
    "# MODEL_NAME = 'model.pt'\n",
    "# if os_path_exists(MODEL_NAME):\n",
    "#     model = torch_load(MODEL_NAME, map_location='cpu')\n",
    "# else:\n",
    "#     model = build_detection_model(cfg)\n",
    "#     torch_save(model, MODEL_NAME)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b780e",
   "metadata": {},
   "source": [
    "# Deliverable: Walk through it once and compute the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e68e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the dataset statistics computed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e496748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images Per GPU: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  train\n",
      "root_classes_count:  {}\n",
      "mean root class number:  0.0\n",
      "sum root class number:  0\n",
      "leaf_classes_count:  {}\n",
      "mean leaf class number:  0.0\n",
      "sum leaf class number:  0\n",
      "all_classes_count:  {}\n",
      "mean all class number:  0.0\n",
      "sum all class number:  0\n",
      "number images:  57723\n"
     ]
    }
   ],
   "source": [
    "from maskrcnn_benchmark.data import make_data_loader\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "        cfg,\n",
    "        mode='train',\n",
    "        is_distributed=False,\n",
    "        start_iter=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "856ebaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log as np_log\n",
    "\n",
    "from maskrcnn_benchmark.data import get_dataset_statistics # calls VisualGenome.get_statistics which calls get_VG_statistics\n",
    "\n",
    "statistics = get_dataset_statistics(cfg)\n",
    "\n",
    "fg_matrix = statistics['fg_matrix']\n",
    "eps = 1e-3\n",
    "\n",
    "pred_dist = np_log(fg_matrix / fg_matrix.sum(2)[:, :, None] + eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c78e0f",
   "metadata": {},
   "source": [
    "# Let's Manually Calculate the correlations from the foreground matrix which is \n",
    "\n",
    "```python\n",
    "fg_matrix = np_zeros((num_obj_classes, num_obj_classes, num_rel_classes), dtype=np_int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140526a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151, 151, 51])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_matrix.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077b39a",
   "metadata": {},
   "source": [
    "# Let's define relation-relation correlation\n",
    "\n",
    "1. Relation Word Embedding Similarity\n",
    "2. Relation Co-occurrence\n",
    "3. Relation covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a475ad3",
   "metadata": {},
   "source": [
    "## 1. Relation Word Embedding Similarity\n",
    "\n",
    "```python\n",
    "for i in idx2label_177:\n",
    "    for j in idx2label_177:\n",
    "        if i == j:\n",
    "            continue\n",
    "        a = wn.synset(label2synset_combined[idx2label_177[i]])\n",
    "        b = wn.synset(label2synset_combined[idx2label_177[j]])\n",
    "        s1 = a.path_similarity(b)\n",
    "        s2 = a.lch_similarity(b) / 3.6375861597263857\n",
    "        s3 = a.wup_similarity(b)\n",
    "        word_sim[:, int(i), int(j)] = [s1, s2, s3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b3c12",
   "metadata": {},
   "source": [
    "# 2. Relation Co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3efa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = fg_matrix.sum((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fddf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol2 = lol[:, None] / lol[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "533ea91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 51])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol2.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bc31",
   "metadata": {},
   "source": [
    "# Naive Relation Augmentation Strategies\n",
    "\n",
    "1. Relation augmentation given pred (like the bar plots)\n",
    "2. Relation augmentation given subj, pred\n",
    "3. Relation augmentation given obj, pred\n",
    "4. Relation augmentation given subj, obj, pred\n",
    "5. Relation augmentation given pred2pred knowledge (word embedding) with an alpha control\n",
    "6. Relation augmentation given pred2pred knowledge (commonsense knowledge - just ConceptNet) with a beta control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbeca1",
   "metadata": {},
   "source": [
    "# 1. Relation augmentation given pred (like the bar plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c156428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_counts = fg_matrix.sum((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea2c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d195c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3024465,    6712,     171,     208,     379,     504,    1829,    1413,\n",
       "          10011,     644,     394,    1603,     397,     460,     565,       4,\n",
       "            809,     163,     157,     663,   67144,   10764,   21748,    3167,\n",
       "            752,     676,     364,     114,     234,   15300,   31347,  109355,\n",
       "            333,     793,     151,     601,     429,      71,    4260,      44,\n",
       "           5086,    2273,     299,    3757,     551,     270,    1225,     352,\n",
       "          47326,    4810,   11059])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10ee5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_freq_pairwise = pred_counts[:, None] / pred_counts[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c83781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import empty_like as torch_empty_like\n",
    "# pred2pred = torch_zeros(51, 51)\n",
    "\n",
    "# for i in range(51):\n",
    "#     for j in range(51):\n",
    "#         preds_ij = fg_matrix[i, j].float()\n",
    "        \n",
    "#         print('before, preds_ij =', preds_ij)\n",
    "# #         print(preds_ij)\n",
    "        \n",
    "#         preds_ij = cov[i, j, :] = preds_ij/preds_ij.sum()\n",
    "#         # PyTorch impl\n",
    "# #         m = preds_ij.mean()\n",
    "# #         s = preds_ij.std(un|biased=False)\n",
    "# #         preds_ij -= m\n",
    "# #         preds_ij /= s\n",
    "#         print('after, preds_ij =', preds_ij)\n",
    "#         print('after, preds_ij.sum() =', preds_ij.sum())\n",
    "#         pred2pred[]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a0ac079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fg_matrix.float().cov((0,1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e75707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_cov(fg_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdaf56",
   "metadata": {},
   "source": [
    "# 3. Relation Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85d4be",
   "metadata": {},
   "source": [
    "# 4. Knowledge-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176dd02",
   "metadata": {},
   "source": [
    "# Question: How did others (like KERN and GB-Net) get their correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c6bc7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95f121db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_data_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed8956b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target, index = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03df4a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3,  20,  49,  49,  57,  58,  59,  97,  99, 105, 111, 115,  77,  78])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = target.extra_fields['labels']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bcb09f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, 50,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 31,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 20,  0, 21,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = target.extra_fields['relation']\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15bb13fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 50, 31, 20, 21])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[relations.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51f2f016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 11, 12, 13, 13])\n",
      "tensor([ 0,  8, 10,  5,  7])\n",
      "(tensor([ 1, 11, 12, 13, 13]), tensor([ 0,  8, 10,  5,  7]))\n"
     ]
    }
   ],
   "source": [
    "subj_id, obj_id = relations_idx = relations.nonzero(as_tuple=True)\n",
    "print(subj_id)\n",
    "print(obj_id)\n",
    "print(relations_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f35a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as os_path_join\n",
    "from os import environ as os_environ\n",
    "\n",
    "DATA_DIR = os_environ['DATA_DIR_VG_RCNN']\n",
    "\n",
    "with open(os_path_join(DATA_DIR, 'visual_genome', 'VG-SGG-dicts-with-attri.json'), 'r') as fin:\n",
    "    scene_graph_meta = json.load(fin)\n",
    "labels = list(scene_graph_meta['label_to_idx'].keys())\n",
    "preds = list(scene_graph_meta['predicate_to_idx'].keys())\n",
    "\n",
    "# TODO: limit to tail classes for freq. Because if I just do a freq based, I'm gonna \n",
    "# make it worse because we are gonna get the higher freq ones for aug triplets.\n",
    "# or just invert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49c2ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2preds = ['_'] + list(scene_graph_meta['idx_to_predicate'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0741242",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2preds_np = np.array(idx2preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "506f8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2preds_np[triplets_new[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e72c0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from maskrcnn_benchmark.structures.image_list import ImageList\n",
    "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
    "\n",
    "\n",
    "\n",
    "def rel_aug(images: ImageList, targets: Tuple[BoxList], num_to_aug: int, strategy: str) -> Tuple[ImageList, Tuple[BoxList]]:\n",
    "    p_rel_all = pred_counts/pred_counts.sum()\n",
    "    if strategy == 'all':\n",
    "        return rel_aug_all(images, targets, p_rel_all, num_to_aug)\n",
    "    else:\n",
    "        raise NotImplementedError(f'rel_aug: strategy={strategy} is not implemented yet')\n",
    "\n",
    "\n",
    "from torch import as_tensor as torch_as_tensor\n",
    "# def rel_aug_all_triplets(triplet: Tuple[Tensor, Tensor, Tensor], num2aug: int, replace: bool) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "def rel_aug_all_triplets(idx_rel: int, num2aug: int, replace: bool) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    '''\n",
    "    Given a single triplet in the form of (subj, rel, obj) and outputs a list of triplets with\n",
    "    the , not including the original. You should append the original.\n",
    "    \n",
    "    Note that the input indices much be singular (one element) but the outputs will have multiple\n",
    "    (specificall num2aug elements).\n",
    "    '''\n",
    "    \n",
    "    # Construct the inverse relation frequency distribution\n",
    "    \n",
    "    n=len(pred_counts)\n",
    "    P_REL_ALL = 1- (pred_counts/pred_counts.sum()).repeat(n, 1)\n",
    "    DIST_RELS_ALL_EXCLUDED_BY = P_REL_ALL.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1) # remove diagonal values\n",
    "\n",
    "#     idxs_subj = []\n",
    "#     idxs_obj = []\n",
    "#     idxs_rel = []\n",
    "    \n",
    "#     idx_subj, idx_rel, idx_obj = triplet\n",
    "    \n",
    "    idx_chosen = DIST_RELS_ALL_EXCLUDED_BY[idx_rel].multinomial(num2aug, replacement=replace)\n",
    "    return idx_chosen\n",
    "#     idxs_subj.append([idx_subj for _ in range(num2aug)])\n",
    "#     idxs_obj.append([idx_obj for _ in range(num2aug)])\n",
    "#     idxs_rel.extend(idx_chosen)\n",
    "#     return torch_as_tensor(idxs_subj), torch_as_tensor(idxs_rel), torch_as_tensor(idxs_obj)\n",
    "\n",
    "def rel_aug_each_instance(triplet: Tuple[Tensor, Tensor, Tensor], num2aug: int, replace: bool) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    '''\n",
    "    Given a single triplet in the form of (subj, rel, obj) and outputs a list of triplets with\n",
    "    the , not including the original. You should append the original.\n",
    "    \n",
    "    Note that the input indices much be singular (one element) but the outputs will have multiple\n",
    "    (specificall num2aug elements).\n",
    "    '''\n",
    "    \n",
    "    # Construct the inverse relation frequency distribution\n",
    "    \n",
    "    n=len(pred_counts)\n",
    "    P_REL_ALL = 1- (pred_counts/pred_counts.sum()).repeat(n, 1)\n",
    "    DIST_RELS_ALL_EXCLUDED_BY = P_REL_ALL.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1) # remove diagonal values\n",
    "\n",
    "    idxs_subj = []\n",
    "    idxs_obj = []\n",
    "    idxs_rel = []\n",
    "    \n",
    "    idx_subj, idx_rel, idx_obj = triplet\n",
    "    \n",
    "    idx_chosen = DIST_RELS_ALL_EXCLUDED_BY[idx_rel].multinomial(num2aug, replacement=replace)\n",
    "    return idx_chosen\n",
    "#     idxs_subj.append([idx_subj for _ in range(num2aug)])\n",
    "#     idxs_obj.append([idx_obj for _ in range(num2aug)])\n",
    "#     idxs_rel.extend(idx_chosen)\n",
    "#     return torch_as_tensor(idxs_subj), torch_as_tensor(idxs_rel), torch_as_tensor(idxs_obj)\n",
    "\n",
    "def rel_aug_all(images: ImageList, targets: Tuple[BoxList], dist_rels_all:Tensor, num2aug: int, replace: bool) -> Tuple[ImageList, Tuple[BoxList]]:\n",
    "    '''\n",
    "    dist_preds_all:Tensor: the frequency distribution of all relations\n",
    "    num2aug:int: \n",
    "    \n",
    "    '''\n",
    "    batch_size = image.size()[0]\n",
    "    # pick up the num_examples draw given distribution of similar \n",
    "    \n",
    "    # 1. Exclude iteratively or better, build cached matrix of excluded distributions by index.\n",
    "    \n",
    "#     for \n",
    "    n = preds_count.size(0)\n",
    "    dist_rels_all_excluded_by = dist_rels_all.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1) # remove diagonal values\n",
    "\n",
    "    # Exclude head relation classes or include tail relation classes.\n",
    "    triplets_augmented = []\n",
    "    for t in triplets:\n",
    "        idx_subj, idx_obj, idx_rel = t\n",
    "        idx_chosen = dist_rels_all_excluded_by[idx_rel].multinomial(num2aug, replacement=replace)\n",
    "        # Actually idx_chosen is the new array of augmentation triplets\n",
    "        # Augment here\n",
    "#         triplets_chosen = \n",
    "        # same objects.\n",
    "        triplets_augmented.extend(triplets_chosen)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "\n",
    "\n",
    "# def mixgen_batch(image, text, num, lam=0.5):\n",
    "#     batch_size = image.size()[0]\n",
    "#     index = np.random.permutation(batch_size)\n",
    "#     for i in range(batch_size):\n",
    "#         # image mixup\n",
    "#         image[i,:] = lam * image[i,:] + (1 - lam) * image[index[i],:]\n",
    "#         # text concat\n",
    "#         text[i] = text[i] + \" \" + text[index[i]]\n",
    "#     return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fcdda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch import no_grad as torch_no_grad, cat as torch_cat, as_tensor as torch_as_tensor\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "\n",
    "\n",
    "# for iteration, (images, targets, _) in tqdm(enumerate(train_data_loader)):\n",
    "#     if iteration > 100: break\n",
    "\n",
    "\n",
    "def augment_targets(images, targets):\n",
    "    # TODO: vectorize\n",
    "#     relations = [target.extra_fields['relation'] for target in targets]\n",
    "#     labels = [target.extra_fields['labels'] for target in targets]\n",
    "    images_augmented = []\n",
    "    targets_augmented = []\n",
    "    for image, target in zip(images.tensors, targets):\n",
    "        relation_old = target.extra_fields['relation']\n",
    "    \n",
    "        idx_subj, idx_obj = idx_rel = relation_old.nonzero(as_tuple=True) # tuple\n",
    "        rels = relation_old[idx_rel]\n",
    "        # Future idea: relation localization. Why are objects locally & globally indexed \n",
    "        # but relations only globally indexed? Because of bbox?\n",
    "\n",
    "        # triplets are represented as the relation map.\n",
    "\n",
    "        # rels is essentially triplets\n",
    "\n",
    "#         idxs_subj = []\n",
    "#         idxs_rel = []\n",
    "#         idxs_obj = []\n",
    "        \n",
    "#         images_from_image = []\n",
    "#         targets_from_target = []\n",
    "        images_augmented.append(image)\n",
    "        targets_augmented.append(target)\n",
    "\n",
    "\n",
    "        for idx_subj_og, rel_og, idx_obj_og in zip(idx_subj, rels, idx_obj):\n",
    "#             idx_subj, idx_rel, idx_obj = rel_aug_all_triplets(triplets_old, 10, True)\n",
    "\n",
    "            # First add old\n",
    "#             images_from_image.append(image)\n",
    "            # TODO: how to append to an ImageList? Use to_image_list to convert torch tensors\n",
    "#             targets_from_target.append(target)\n",
    "            # TODO: how to append to targets? It's a list of boxlists. So might be easier.\n",
    "\n",
    "#             triplets_new = rel_aug_all_triplets(triplet_old, 10, True)\n",
    "#             triplets_new = rel_aug_all_triplets(rel_og, 10, True)\n",
    "            rels_new = rel_aug_all_triplets(rel_og, 10, True)\n",
    "            print(len(rels_new))\n",
    "            for rel_new in rels_new:\n",
    "#                 images_from_image.append(image)\n",
    "                images_augmented.append(image)\n",
    "                \n",
    "                # NOTE on meditation and working and reducing anxiety and zen: remember\n",
    "                # doing math proofs in the downstairs apartment? How calm and focused I was?\n",
    "                # like Xin Ru Zhi Shui.\n",
    "                \n",
    "                # Only idx_rel is new here.\n",
    "#                 idx_obj, idx_rel, idx_subj = triplet_new\n",
    "                \n",
    "                # Triplet to Map\n",
    "                relation_new = relation_old.detach().clone()\n",
    "#                 target_new = target.detach().clone()\n",
    "                target_new = deepcopy(target)\n",
    "                relation_new[idx_subj_og, idx_subj_og] = rel_new\n",
    "                target_new.extra_fields['relation'] = relation_new\n",
    "#                 targets_from_target.append(target_new)\n",
    "#                 targets_from_target.append(target_new)\n",
    "                targets_augmented.append(target_new)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return to_image_list(images_augmented), targets_augmented\n",
    "                \n",
    "                # Do we really need to recreate relation_old from zeros? I don't think so. \n",
    "                \n",
    "#                 (x == 0).nonzero()\n",
    "#                 x = torch.empty(5, 5).random_(3)\n",
    "#                 idx = x.nonzero()\n",
    "#                 y = torch.zeros(5, 5)\n",
    "#                 y[idx[:,0], idx[:,1]] = x[idx[:, 0], idx[:, 1]]\n",
    "#                 print((x==y).all())\n",
    "                \n",
    "#             idxs_subj.append(idx_subj)\n",
    "#             idxs_rel.append(idx_rel)\n",
    "#             idxs_obj.append(idx_obj)\n",
    "            \n",
    "            # For single-label augmentation, each iteration in this loop is an \"extra\" image and target\n",
    "            \n",
    "\n",
    "#         idxs_subj = torch_cat(idxs_subj)\n",
    "#         idxs_rel = torch_cat(idxs_rel)\n",
    "#         idxs_obj = torch_cat(idxs_obj)\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "        # TODO: convert matrices back into map like relation_old [num_obj, num_obj]\n",
    "        # currently, these maps are singular in terms of relations. We cannot have multiple.\n",
    "        # perhaps we need multilabel anyway? One way to do this is to borrow from the MLSGG codebase.\n",
    "        # Alternatively, output multiple maps with the same images and targets but with different gt map.\n",
    "        \n",
    "        # But doesn't single-label run into problem of bad gt answers? Can both be correct? I'm not so sure.\n",
    "        \n",
    "        \n",
    "#         target.extra_fields['relation'] = \n",
    "\n",
    "#     return targets\n",
    "\n",
    "    # where is the ground truth relation labels? There's definitely some in get_dataset_statistics\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93d1dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count_dict = dict(zip(idx2preds[1:], pred_counts[1:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e1fe5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# mylist = [key for key, val in pred_count_dict.items() for _ in range(val)]\n",
    "plt.figure(figsize=(16, 12))\n",
    "names, counts = zip(*pred_count_dict.items())\n",
    "plt.bar(names, counts)\n",
    "# plt.hist(mylist, bins=20)\n",
    "# plt.hist(pred_counts)\n",
    "# plt.hist(pred_counts.tolist())\n",
    "# plt.tight_layout()\n",
    "plt.savefig('preds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7a09699",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rel_all = pred_counts/pred_counts.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa37151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sort as torch_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4fba0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_counts_sorted, pred_counts_sorted_indices = torch_sort(pred_counts[1:], descending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea16b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_counts_sorted, pred_counts_sorted_indices = torch_sort(pred_counts, descending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7573092f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([      4,      44,      71,     114,     151,     157,     163,     171,\n",
       "            208,     234,     270,     299,     333,     352,     364,     379,\n",
       "            394,     397,     429,     460,     504,     551,     565,     601,\n",
       "            644,     663,     676,     752,     793,     809,    1225,    1413,\n",
       "           1603,    1829,    2273,    3167,    3757,    4260,    4810,    5086,\n",
       "           6712,   10011,   10764,   11059,   15300,   21748,   31347,   47326,\n",
       "          67144,  109355, 3024465])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dd247ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 39,\n",
       " 42,\n",
       " 44,\n",
       " 45,\n",
       " 47}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pred_counts_sorted_indices[:30].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2eba881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4,  44,  71, 114, 151, 157, 163, 171, 208, 234, 270, 299, 333, 352,\n",
       "        364, 379, 394, 397, 429, 460, 504, 551, 565, 601, 644, 663, 676, 752,\n",
       "        793, 809])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counts[pred_counts_sorted_indices[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f9cb2e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4,  44,  71, 114, 151, 157, 163, 171, 208, 234, 270, 299, 333, 352,\n",
       "        364, 379, 394, 397, 429, 460, 504, 551, 565, 601, 644, 663, 676, 752,\n",
       "        793, 809])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_bottom_k = pred_counts_sorted_indices[:30]\n",
    "\n",
    "pred_counts[indices_bottom_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "66b37d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46,  7, 11,  6, 41, 23, 43, 38, 49, 40,  1,  8, 21, 50, 29, 22, 30, 48,\n",
       "        20, 31,  0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counts_sorted_indices[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0f901faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   601,   1603,    460,   3167,    809,    333,    793,  47326,    352,\n",
       "         21748,    504,  10011,    157,    208,    752,   1225,   5086,   4260,\n",
       "           565,     71,    270,   1829,    429,      4,   2273, 109355,    676,\n",
       "           364,    163,    151])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counts[~indices_bottom_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1fdcf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flying in'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2preds_np[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c99a1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count_dict = dict(zip(idx2preds_np[1:][pred_counts_sorted_indices], pred_counts[1:][pred_counts_sorted_indices].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "755e74e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flying in': 4,\n",
       " 'says': 44,\n",
       " 'playing': 71,\n",
       " 'made of': 114,\n",
       " 'painted on': 151,\n",
       " 'growing on': 157,\n",
       " 'from': 163,\n",
       " 'across': 171,\n",
       " 'against': 208,\n",
       " 'mounted on': 234,\n",
       " 'walking in': 270,\n",
       " 'to': 299,\n",
       " 'on back of': 333,\n",
       " 'watching': 352,\n",
       " 'lying on': 364,\n",
       " 'along': 379,\n",
       " 'between': 394,\n",
       " 'covered in': 397,\n",
       " 'part of': 429,\n",
       " 'covering': 460,\n",
       " 'and': 504,\n",
       " 'using': 551,\n",
       " 'eating': 565,\n",
       " 'parked on': 601,\n",
       " 'belonging to': 644,\n",
       " 'hanging from': 663,\n",
       " 'looking at': 676,\n",
       " 'laying on': 752,\n",
       " 'over': 793,\n",
       " 'for': 809,\n",
       " 'walking on': 1225,\n",
       " 'attached to': 1413,\n",
       " 'carrying': 1603,\n",
       " 'at': 1829,\n",
       " 'standing on': 2273,\n",
       " 'in front of': 3167,\n",
       " 'under': 3757,\n",
       " 'riding': 4260,\n",
       " 'wears': 4810,\n",
       " 'sitting on': 5086,\n",
       " 'above': 6712,\n",
       " 'behind': 10011,\n",
       " 'holding': 10764,\n",
       " 'with': 11059,\n",
       " 'near': 15300,\n",
       " 'in': 21748,\n",
       " 'of': 31347,\n",
       " 'wearing': 47326,\n",
       " 'has': 67144,\n",
       " 'on': 109355}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bc5b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count_dict_1000 = {k:v for (k,v) in pred_count_dict.items() if v <= 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67babf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_count_dict_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "678f4cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 19, 47, 29, 21, 28, 49, 20,  7,  0, 39, 48, 37, 42, 22, 40,  5, 10,\n",
       "         6, 45, 15, 32, 23, 24, 18,  8, 34, 13, 43,  4, 12, 35, 11,  9,  3, 25,\n",
       "        46, 31, 41, 44, 27,  2,  1, 16, 17, 33, 26, 36, 38, 14])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counts_sorted_indices[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55363bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_test = [15, 39, 37, 27, 34, 18, 17, 2, 3, 28, 45, 42, 32, 47, 26, 4, 10, 12, 36, 13, 5, 44, 14, 35, 9, 19, 25, 24, 33, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efda06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count_dict = dict(zip(idx2preds_np[1:][indices_test], pred_counts[1:][indices_test].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4fdff173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'for': 809,\n",
       " 'sitting on': 5086,\n",
       " 'riding': 4260,\n",
       " 'mounted on': 234,\n",
       " 'parked on': 601,\n",
       " 'hanging from': 663,\n",
       " 'growing on': 157,\n",
       " 'against': 208,\n",
       " 'along': 379,\n",
       " 'near': 15300,\n",
       " 'walking on': 1225,\n",
       " 'under': 3757,\n",
       " 'over': 793,\n",
       " 'wearing': 47326,\n",
       " 'made of': 114,\n",
       " 'and': 504,\n",
       " 'carrying': 1603,\n",
       " 'covering': 460,\n",
       " 'playing': 71,\n",
       " 'eating': 565,\n",
       " 'at': 1829,\n",
       " 'walking in': 270,\n",
       " 'flying in': 4,\n",
       " 'part of': 429,\n",
       " 'between': 394,\n",
       " 'has': 67144,\n",
       " 'lying on': 364,\n",
       " 'looking at': 676,\n",
       " 'painted on': 151,\n",
       " 'from': 163}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bd49f56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # from maskrcnn_benchmark.structures.image_list import ImageList\n",
    "# # to_image_list = ImageList.to_image_list\n",
    "\n",
    "# for iteration, (images, targets, _) in tqdm(enumerate(train_data_loader)):\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     images_new, targets_new = augment_targets(images, targets)\n",
    "# #     images_new = targets.to_image_list(images_new)\n",
    "# #     import pdb; pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15ff7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# MixGen: A New Multi-Modal Data Augmentation\n",
    "# https://arxiv.org/abs/2206.08358\n",
    "# Apache-2.0 License, Copyright 2022 Amazon\n",
    "# \"\"\"\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def mixgen(image, text, num, lam=0.5):\n",
    "#     # default MixGen\n",
    "#     for i in range(num):\n",
    "#         # image mixup\n",
    "#         image[i,:] = lam * image[i,:] + (1 - lam) * image[i+num,:]\n",
    "#         # text concat\n",
    "#         text[i] = text[i] + \" \" + text[i+num]\n",
    "#     return image, text\n",
    "\n",
    "# def mixgen(image, text, num, lam=0.5):\n",
    "#     # default MixGen\n",
    "#     for i in range(num):\n",
    "#         # image mixup\n",
    "#         image[i,:] = lam * image[i,:] + (1 - lam) * image[i+num,:]\n",
    "#         # text concat\n",
    "#         text[i] = text[i] + \" \" + text[i+num]\n",
    "#     return image, text\n",
    "\n",
    "# >>> from random import choices\n",
    "# >>> population = [1, 2, 3, 4, 5, 6]\n",
    "# >>> weights = [0.1, 0.05, 0.05, 0.2, 0.4, 0.2]\n",
    "# >>> choices(population, weights)\n",
    "# [4]\n",
    "\n",
    "\n",
    "\n",
    "# # https://discuss.pytorch.org/t/torch-equivalent-of-numpy-random-choice/16146/14\n",
    "# a = np.array([1, 2, 3, 4])\n",
    "# p = np.array([0.1, 0.1, 0.1, 0.7])\n",
    "# n = 2\n",
    "# replace = True\n",
    "# b = np.random.choice(a, p=p, size=n, replace=replace)\n",
    "\n",
    "# # Equivalent to\n",
    "\n",
    "# a = torch.tensor([1, 2, 3, 4])\n",
    "# p = torch.tensor([0.1, 0.1, 0.1, 0.7])\n",
    "# n = 2\n",
    "# replace = True\n",
    "# idx = p.multinomial(num_samples=n, replacement=replace)\n",
    "# b = a[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67dbdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.data import Dataset\n",
    "\n",
    "# class CachedDataset(Dataset):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d7ae440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import Embedding\n",
    "\n",
    "# emb_obj = Embedding(151, 100)\n",
    "# # emb_relation = Embedding(51, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1d1bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import cat as torch_cat\n",
    "\n",
    "# subj_label_gt_concat = torch_cat(subj_label_gt, dim=0)\n",
    "# obj_label_gt_concat = torch_cat(obj_label_gt, dim=0)\n",
    "# relation_gt_concat = torch_cat(relation_gt, dim=0)\n",
    "# union_features_all_concat = torch_cat(union_features_all, dim=0)\n",
    "# roi_features_all_concat = torch_cat(roi_features_all, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dea19d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subj_label_gt_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f15a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_one_hot = emb_obj(subj_label_gt_concat)\n",
    "# obj_one_hot = emb_obj(obj_label_gt_concat)\n",
    "# # relation_one_hot = emb_relation(relation_gt_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d446cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfa58e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import DataFrame\n",
    "\n",
    "# df = DataFrame(columns=['relation_pred', 'subj_label_gt', 'obj_label_gt', 'relation_gt', 'union_features', 'roi_features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a659d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union_features_all_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c6c8f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_gt_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b007ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = torch_cat((subj_one_hot, obj_one_hot, union_features_all_concat, roi_features_all_concat), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea615e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_label_gt_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10474688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(subj_label_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f19d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(roi_features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3634509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['obj_label_gt'] = \n",
    "# df['subj_label_gt'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6240cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['roi_features'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a600688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f5e53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 151\n",
    "# embedding_size = 10\n",
    "\n",
    "# embedding = nn.Embedding(num_classes, embedding_size)\n",
    "\n",
    "# class_vector = torch.tensor([1, 0, 3, 3, 2])\n",
    "\n",
    "# embedded_classes = embedding(class_vector)\n",
    "# embedded_classes.size() # => torch.Size([5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "caaf8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import cat as torch_cat\n",
    "# from torch.nn import Module, Linear\n",
    "# from torch.optim import Adam\n",
    "\n",
    "\n",
    "# class LogisticRegressionGD(Module):\n",
    "#     def __init__(self, input_dim, output_dim, embedding_size):\n",
    "#         super().__init__()\n",
    "#         self.emb = Embedding(num_classes, embedding_size)\n",
    "#         self.linear = Linear(input_dim, output_dim)\n",
    "        \n",
    "#     def forward(self, subj_label_gt, obj_label_gt, union_features, roi_features):\n",
    "#         subj = self.emb(subj_label_gt)\n",
    "#         obj = self.emb(obj_label_gt)\n",
    "#         x = torch_cat(subj, obj, union_features, roi_features)\n",
    "#         return self.linear(x)\n",
    "    \n",
    "# loss_fn = BCEWithLogitsLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# model_gd = LogisticRegressionGD()\n",
    "# # preds = model()\n",
    "# # loss = loss_fn(input, target)\n",
    "# # loss.backward()\n",
    "\n",
    "# # for epochs\n",
    "# for inputy, target in dataset:\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model_gd(inputy)\n",
    "#     loss = loss_fn(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "906391d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegressionGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e01b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "\n",
    "# Result = minimize(fun = logLikelihoodLogit, \n",
    "#                                  x0 = np.array([-.1, -.03, -.01, .44, .92, .53,1.8, .71]), \n",
    "#                                  args = (mX, vY),\n",
    "#                                  method = 'TNC',\n",
    "#                                  jac = likelihoodScore);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f55d4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#         for relation, label in zip(relations, labels)\n",
    "#             subj_label, obj_label = label[subj_idx], label[obj_idx]\n",
    "        \n",
    "#         print(f'#relations = {len(relations[0])}')\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "\n",
    "#         subj_idx, obj_idx = relations_idxs = [relation.nonzero(as_tuple=True) for relation in relations]\n",
    "#         relations_idxs = [relation.nonzero(as_tuple=True) for relation in relations]\n",
    "        \n",
    "#         subj_idx = [relations_idx[0] for relations_idx in relations_idxs]\n",
    "#         obj_idx = [relations_idx[1] for relations_idx in relations_idxs]\n",
    "#         result, rel_pair_idxs, roi_features, union_features = model(images, targets)\n",
    "    \n",
    "#         for pair_idx in rel_pair_idxs:\n",
    "\n",
    "# #             import pdb; pdb.set_trace()\n",
    "\n",
    "# #             subj_idx, obj_idx = relations_idx = relation.nonzero(as_tuple=True)\n",
    "\n",
    "# #             print(f'#relations = {len(relation)}')\n",
    "\n",
    "#             #             import pdb; pdb.set_trace()\n",
    "#             # There are more proposal relations than gt relations\n",
    "#             relation_label = relation[relations_idx]\n",
    "#             # look up: if match. Otherwise 0\n",
    "            \n",
    "            \n",
    "#             subj_label, obj_label = label[subj_idx], label[obj_idx]\n",
    "\n",
    "#             subj_label_gt.append(subj_label.cpu())\n",
    "#             obj_label_gt.append(obj_label.cpu())\n",
    "#             relation_gt.append(relation_label.cpu())\n",
    "    \n",
    "\n",
    "#         for relation, label, pair_idx in zip(relations, labels, rel_pair_idxs):\n",
    "            \n",
    "# #             import pdb; pdb.set_trace()\n",
    "            \n",
    "#             subj_idx, obj_idx = relations_idx = relation.nonzero(as_tuple=True)\n",
    "#             relation_idx_ = relation.nonzero()\n",
    "            \n",
    "#             print(f'#relations = {len(relation)}')\n",
    "            \n",
    "#             relation_label = relation[relations_idx]\n",
    "            \n",
    "#             import pdb; pdb.set_trace()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Proposals\n",
    "#             rel_pair_idx[]\n",
    "            \n",
    "#             for propsed_pair_id in rel_pair_idx:\n",
    "#                 if propsed_pair_id\n",
    "                \n",
    "#             label = index_select(, 1, 0)\n",
    "            \n",
    "#             label_total_with_0 = torch_where(rel_idx_proposed=rel_idx_gt,)\n",
    "\n",
    "\n",
    "#             subj_label_gt.append(subj_label.cpu())\n",
    "#             obj_label_gt.append(obj_label.cpu())\n",
    "#             relation_gt.append(relation_label.cpu())\n",
    "\n",
    "#         roi_features_all.append(roi_features.cpu())\n",
    "#         union_features_all.append(union_features.cpu())\n",
    "        \n",
    "#         torch_cat((roi_features, union_features), dim=-1)\n",
    "\n",
    "\n",
    "#         del subj_label, obj_label, relation_label, roi_features, union_features, images, targets\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgb",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
